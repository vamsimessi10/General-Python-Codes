{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "865\n",
      "865\n",
      "1515\n",
      "1645\n",
      "2930\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import urllib\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import urllib\n",
    "import urllib.request\n",
    "import html.parser\n",
    "import requests\n",
    "from requests.exceptions import HTTPError\n",
    "from socket import error as SocketError\n",
    "from http.cookiejar import CookieJar\n",
    "\n",
    "#Original URl\n",
    "\n",
    "url='https://en.wikipedia.org/wiki/'\n",
    "\n",
    "#Source URL\n",
    "\n",
    "\n",
    "\n",
    "source_url = 'https://en.wikipedia.org/wiki/2019%E2%80%9320_Wuhan_coronavirus_outbreak'\n",
    "with urllib.request.urlopen(source_url) as response:\n",
    "    soup = BeautifulSoup(response)\n",
    "\n",
    "#Unclean URLS - As the name suggests - All the URLS with href \n",
    "\n",
    "uncleaned_urls = set()\n",
    "for a_tag in soup.body.find_all('a'):\n",
    "     if 'href' in a_tag.attrs:\n",
    "        url = a_tag.attrs['href']\n",
    "        uncleaned_urls.add(url)\n",
    " \n",
    " \n",
    " #Normalized Urls\n",
    " \n",
    "normalized_urls = set()\n",
    "for unclean_url in uncleaned_urls:\n",
    "    \n",
    "    # First convert the URL to something absolute by \"joining\"\n",
    "    # it with the source URL we got it from\n",
    "    abs_url = urllib.parse.urljoin(source_url, unclean_url)\n",
    "    \n",
    "    # Now let's remove the fragment part \n",
    "    # the part after the # in the URL\n",
    "    # We're going to leave in query strings because although they might\n",
    "    # resolve to the same resource, we might not be sure.\n",
    "    defrag_url = urllib.parse.urldefrag(abs_url).url\n",
    "    \n",
    "    # Now we can add it to our set\n",
    "    normalized_urls.add(defrag_url)\n",
    "    \n",
    "    \n",
    " #Filtering for webpage Urls\n",
    " \n",
    "webpage_urls = set()\n",
    "content_types = set()\n",
    "for url in normalized_urls:\n",
    "    try:\n",
    "        with urllib.request.urlopen(url, None, 5) as request:\n",
    "            content_types.add(request.headers.get_content_type())\n",
    "            if request.headers.get_content_type()  == 'text/html':\n",
    "                webpage_urls.add(url)\n",
    "    except:\n",
    "        pass #If there is a 404 error or the like, just go to the next url\n",
    "        \n",
    " # Mapping the same domain urls9from Source URl) with the webpage_urls \n",
    " \n",
    "same_domain_urls = set()\n",
    "parsed_source = urllib.parse.urlparse(source_url)\n",
    "for url in webpage_urls:\n",
    "    parsed_target = urllib.parse.urlparse(url)\n",
    "    if parsed_target.netloc == parsed_source.netloc:\n",
    "        same_domain_urls.add(url)\n",
    "        \n",
    "print(len(same_domain_urls))\n",
    "\n",
    "\n",
    "#Checking if the number of urls decreased from uncleaned to same_domain \n",
    "\n",
    "print(len(same_domain_urls))\n",
    "print(len(webpage_urls))\n",
    "print(len(normalized_urls))\n",
    "print(len(uncleaned_urls))\n",
    "    \n",
    "import lxml.html\n",
    "import pandas as pd\n",
    "\n",
    "# converting the set of same_domain_urls to a Data Frame\n",
    "\n",
    "dt= pd.DataFrame(same_domain_urls)\n",
    "\n",
    "#Renaming\n",
    "dt.rename({0:'Same Domain URLs'}, axis= 1, inplace=True)\n",
    "\n",
    "#New blank column for Titles\n",
    "\n",
    "dt[\"Title\"] = \"\"\n",
    "\n",
    "#Loop that runs to fetch the title and adds it to the Title column \n",
    "\n",
    "for i in range(len(same_domain_urls)):\n",
    "    url = dt['Same Domain URLs'].iloc[i]\n",
    "    r = requests.get(url)\n",
    "    html_content = r.text\n",
    "    soup = BeautifulSoup(html_content, 'lxml')\n",
    "    dt['Title'].iloc[i] = soup.title.string\n",
    "    \n",
    "dt \n",
    "\n",
    "#Column Reaarrangement\n",
    "\n",
    "dt.rename({'Same Domain URLs':'url_target','Title':'page_title_target'}, axis= 1, inplace=True)\n",
    "\n",
    "dt[\"url_source\"] = \"\"\n",
    "\n",
    "dt[\"url_source\"] = \"https://en.wikipedia.org/wiki/2019%E2%80%9320_Wuhan_coronavirus_outbreak\"\n",
    "\n",
    "dt[\"dummy\"]= dt[\"url_source\"]\n",
    "\n",
    "dt[\"url_source\"] = dt[\"page_title_target\"]\n",
    "\n",
    "dt[\"page_title_target\"] = dt[\"url_target\"]\n",
    "\n",
    "dt[\"url_target\"] = dt[\"dummy\"]\n",
    "\n",
    "dt.drop([\"dummy\"], axis=1)\n",
    "\n",
    "dt.rename({'url_target':'url_source','page_title_target':'url_target','url_source':'page_title_target'}, axis= 1, inplace=True)\n",
    "\n",
    "dt\n",
    "\n",
    "df = dt.head(100)\n",
    "\n",
    "dr=df.drop([\"dummy\"], axis=1)\n",
    "\n",
    "\n",
    "#Checking for the current working directory\n",
    "import os\n",
    "os. getcwd()\n",
    "\n",
    "#Export CSV\n",
    "\n",
    "export_csv = dr.to_csv (r'C:\\Users\\vamsi\\Downloads\\crawl.csv', index = None, header=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
